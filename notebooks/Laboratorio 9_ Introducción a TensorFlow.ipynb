{"cells":[{"cell_type":"markdown","metadata":{"id":"dtiC5o0m6HLi"},"source":["# **Laboratorio 9:** Introducción a TensorFlow\n","**Programa:** [Bootcamp en Visión Artificial para los ODS](https://github.com/EdwinTSalcedo/Bootcamp-Computer-Vision-for-the-SDGs) - **Autor:** [Edwin Salcedo](https://github.com/EdwinTSalcedo)\n","\n","¡Bienvenidos! Desde este laboratorio, nos enfocaremos en modelar arquitecturas de Deep Learning para tareas de Visión Artificial. La herramienta que usaremos es [Tensorflow](https://www.tensorflow.org/), la cual es una librería de código abierto para desarrollar pipelines de adquisición y procesamiento de datos, implementar modelos predictivos, y desplegar modelos para inferencia. Este framework fue lanzado en el 2015 por Google Brain; sin embargo, el 2019, Google lanzo TensorFlow 2.0 con funcionalidades actualizadas, un ecosistema mas compatible, y una mejor API de Keras.\n","\n","Las estructura de datos principal en TensorFlow es el *tensor* y puedes pensar en este como estructuras de datos similares a los arrays de Numpy, por lo que TensorFlow en muchos sentidos comparte características y funcionalidades con esta librería. Después de todo, los arrays multidimensionales en Numpy son también tensores. Una de las principales ventajas de Tensorflow es que los modelos desarrollados con este framework tienen posibilidad de acloparse a infinidad de lenguajes de programación, dispositivos móviles y dispositivos embebidos.\n","\n","<center><img src='https://media.giphy.com/media/QyJTDR8VkUtyKHNPm9/giphy.gif' width='30%'></center>\n","\n","Registra los datos de tu equipo en esta sección al finalizar el laboratorio. \n","\n","**Nombre de equipo:**\n","\n","**Miembros de equipo:**\n","- << nombre >> << apellido >> (Contribución sobre el 25%)\n","- << nombre >> << apellido >> (Contribución sobre el 25%)\n","- << nombre >> << apellido >> (Contribución sobre el 25%)\n","- << nombre >> << apellido >> (Contribución sobre el 25%) \n"]},{"cell_type":"markdown","metadata":{"id":"tRfFDeb86HLr"},"source":["## 1. Tensores en TensorFlow\n","\n","Como vimos anteriormente, los cálculos en redes neuronales son solo un conjunto de operaciones de álgebra lineal usando *tensores*, como una generalización de las matrices. La estructura de datos fundamental para las redes neuronales son los tensores y TF (así como casi todos los demás frameworks de aprendizaje profundo) están orientados a usar estas estructuras de datos. Puedes entender que un vector es un tensor unidimensional, una matriz es un tensor bidimensional, y una matriz con elementos de tres índices es un tensor tridimensional. Un tensor puede guardar información de cualquier archivo o conjunto de datos que haya sido convertido a valores numéricos. Por ejemplo, un tensor podría contener valores numéricos de los precios de casas, una imagen, o todas las palabras de un libro. \n","\n","<center>\n","<img src='https://drive.google.com/uc?id=1chsNh-3TlvxsLzHUE59aZ8XpAiHunZri' width='60%'>\n","</center>\n","\n","La principal diferencia entre los tensores y las matrices NumPy es que los tensores se pueden usar en GPUs ([Graphical Processing Units](https://en.wikipedia.org/wiki/Graphics_processing_unit)) y TPUs ([Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)). El beneficio de ejecutar operaciones con tensores en la GPU/TPU y no en la CPU es la rapidez de los calculos, lo que significa que, si quisiéramos encontrar patrones en las representaciones numéricas de nuestros datos, las podremos encontrar en la mitad (o menos) del tiempo que tomaría encontrarlos usando CPUs ([Central Processing Units](https://en.wikipedia.org/wiki/Central_processing_unit)). Aunque tanto las las GPUs como las TPUs son caras y poco accesibles, Colab nos permite acceder a hardware con GPUs y TPUs de forma libre y con muchas opciones para entrenar modelos. \n","\n","Con los conceptos básicos cubiertos, es hora de explorar cómo podemos usar TF para trabajar con tensores. Primero iniciaremos importando TensorFlow. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud0kHpYf6HLs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654731137187,"user_tz":240,"elapsed":3025,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"53533097-3709-4ef0-b24e-f88b454aeeb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.8.2\n"]}],"source":["# Importar Tensorflow y mostrar su versión\n","import tensorflow as tf\n","\n","print('TensorFlow version:', tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"ROaHv2qZ6HLs"},"source":["En general, uno no crea tensores por si mismo sino que programa a TF para crearlos. Esto debido a que TF tiene módulos integrados (como `tf.io` y tf.data) que pueden leer datasets y convertirlos automáticamente en tensores, para que después los modelos de redes neuronales los procesen por nosotros. Sin embargo, es muy importante entender como trabajar con ellos y comprender como funcionan. "]},{"cell_type":"markdown","source":["## 2. Creación de Tensores"],"metadata":{"id":"a909BMoM9H3h"}},{"cell_type":"markdown","source":["Seis de las formas mas frecuentes para crear tensores se realizan con las funciones `tf.constant()`, `tf.Variable()`, `tf.fill()`, `tf.ones()`, `tf.zeros()` y `tf.random`. Las dos primeras permiten crear tensores con dimensión y valores variables; sin embargo, ambas se diferencian en que `tf.constant()` no permite modificar el tensor después de la creación, mientras que `tf.variable()` si. Entonces, podemos considerar a `tf.constant()` como las variables constantes presentes en la mayoría de lenguajes de programación. \n","\n","Por otro lado `tf.fill()`, `tf.ones()` y `tf.zeros()` permiten crear tensores con un valor predeterminado para todos los elementos del tensor. Finalmente, `tf.random` permite crear tensores con valores aleatorios. En las siguientes celdas exploraremos estos metodos."],"metadata":{"id":"Y5t6pD7NIwaP"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hULACgOb6HLt","outputId":"76c7cd9a-0134-45d2-8aa6-30841d3cd0fa","executionInfo":{"status":"ok","timestamp":1654731148031,"user_tz":240,"elapsed":2,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(12, shape=(), dtype=int32)\n","N. dimensiones:  0\n"]}],"source":["# Crear un escalar (Tensor en la dimensión 0). Un tensor puede tener muchas \n","# dimensiones y la dimensión 0 sera para crear un número escalar independiente. \n","scalar = tf.constant(12)\n","# Mostrar tensor\n","print(scalar)\n","# Comprobar el número de dimensiones de un tensor (ndim significa número de dimensiones)\n","print(\"N. dimensiones: \", scalar.ndim)"]},{"cell_type":"code","source":["# Crear un vector (Tensor de 1 dimensión)\n","vector = tf.constant([12, 12])\n","print(vector)\n","print(\"N. dimensiones: \", vector.ndim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alecHEaE-vcv","executionInfo":{"status":"ok","timestamp":1654731155120,"user_tz":240,"elapsed":337,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"186ae9c4-4d21-46fa-e0d1-0709d450f21d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([12 12], shape=(2,), dtype=int32)\n","N. dimensiones:  1\n"]}]},{"cell_type":"code","source":["# Crear una matriz (Tensor de 2 dimensiónes)\n","matrix = tf.constant([[4.6, 2.7],\n","                      [7.9, 10.1]])\n","print(matrix)\n","print(\"N. dimensiones: \", matrix.ndim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4_BwJzRACj6","executionInfo":{"status":"ok","timestamp":1654731398081,"user_tz":240,"elapsed":311,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"43bcf476-05ad-4cf6-a6db-2ff3d874c36b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 4.6  2.7]\n"," [ 7.9 10.1]], shape=(2, 2), dtype=float32)\n","N. dimensiones:  2\n"]}]},{"cell_type":"markdown","source":["Como puedes notar, TF crea tensores con un tipo de datos `int32` o `float32`. Esto se conoce como precisión de 32 bits, lo que significa que TF hara uso de 32 valores binarios para guardar cada elemento del tensor. Cuanto mayor sea el número de bits, más preciso será el número a guardarse en memoria. Además que mientras mayor sea el número de bits, más espacio requerirá un tensor para guardarse en su equipo. Esta vez reduciremos la precisión a 16 bits por elemento del tensor."],"metadata":{"id":"TM0CXER9AVxD"}},{"cell_type":"code","source":["# Crear otra matriz y definir el tipo de datos\n","another_matrix = tf.constant([[5.3, 7.5],\n","                              [3.1, 2.],\n","                              [8.54, 2.4]], dtype=tf.float16) # Especificar el tipo de dato con 'dtype'\n","print(another_matrix)\n","# Similar a Numpy, la propiedad dtype permite ver el tipo de dato de cada elemento en el tensor\n","print(\"Tipo de dato:\", another_matrix.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kk_U0xmSAVLj","executionInfo":{"status":"ok","timestamp":1654731401018,"user_tz":240,"elapsed":343,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"0f597e07-e8f0-4a59-8e18-156767bc1413"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[5.3  7.5 ]\n"," [3.1  2.  ]\n"," [8.54 2.4 ]], shape=(3, 2), dtype=float16)\n","Tipo de dato: <dtype: 'float16'>\n"]}]},{"cell_type":"code","source":["# ¡Finalmente, creemos un tensor de 3 dimensiones!\n","tensor = tf.constant([[[1, 2, 3],\n","                       [4, 5, 6]],\n","                      [[7, 8, 9],\n","                       [10, 11, 12]],\n","                      [[13, 14, 15],\n","                       [16, 17, 18]]])\n","print(tensor)\n","print(\"N. dimensiones: \", tensor.ndim)\n","\n","# Aprovecharemos este tensor para imprimir mas atributos del tensor\n","# Recuperar la forma del tensor\n","print(\"Forma del tensor:\", tensor.shape)\n","# Recuperar el número de elementos en el tensor. Para esto usaremos la función \n","# tf.size(); sin embargo, esta función devuelve un tensor con la dimensión 0\n","# por lo que sera necesario convertirlo a un array de Numpy para ver directamente su valor\n","print(\"Número de elementos en el tensor:\", tf.size(tensor).numpy()) # .numpy() convierte el tensor a un array de NumPy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nk83uj9DN83","executionInfo":{"status":"ok","timestamp":1654731586058,"user_tz":240,"elapsed":333,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"8b2e518a-0270-4938-ad92-f1146982c338"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[ 1  2  3]\n","  [ 4  5  6]]\n","\n"," [[ 7  8  9]\n","  [10 11 12]]\n","\n"," [[13 14 15]\n","  [16 17 18]]], shape=(3, 2, 3), dtype=int32)\n","N. dimensiones:  3\n","Forma del tensor: (3, 2, 3)\n","Número de elementos en el tensor: 18\n"]}]},{"cell_type":"markdown","source":["La diferencia entre `tf.variable()` y `tf.constant()` es que los tensores creados con `tf.constant()` son inmutables (no se pueden cambiar después de su definición), mientras que los tensores creados con `tf.variable()` son mutables (se pueden cambiar). Los ejemplos, planteados arriba son aplicables también a los tensores creados con `tf.variable()`. Desde este punto, nos concentraremos en los tensores variables. "],"metadata":{"id":"jxxnlZJvOVvR"}},{"cell_type":"code","source":["# Veamos la estructura creada para ambos tipos de tensor con los mismos datos\n","variable_tensor = tf.Variable([10, 7])\n","constant_tensor = tf.constant([10, 7])\n","variable_tensor, constant_tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"miMh6sdQOqYH","executionInfo":{"status":"ok","timestamp":1654731592572,"user_tz":240,"elapsed":306,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"895aa21c-4c42-4a73-8d1e-bb008bc71f34"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([10,  7], dtype=int32)>,\n"," <tf.Tensor: shape=(2,), dtype=int32, numpy=array([10,  7], dtype=int32)>)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Cambiar el valor de un tensor\n","variable_tensor[0].assign(7)\n","variable_tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rx0Ldip8P-FV","executionInfo":{"status":"ok","timestamp":1654731594652,"user_tz":240,"elapsed":303,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"0d91e027-6dc5-42f4-83d9-7181f6a2af35"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([7, 7], dtype=int32)>"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["La anterior operación generaría errores si la ejecutaramos con `constant_tensor`. Para definir que tipo de tensor usar, sera importante identificar si el tensor sera modificado en algún punto del programa, en ese caso siempre se sugiere usar `tf.Variable()`. Por otra parte, la siguiente celda muestra algunas maneras para iterar en tensores de TF. "],"metadata":{"id":"xtIrB0_UQUeZ"}},{"cell_type":"code","source":["for i in tensor:\n","  for j in i:\n","    for k in j:\n","      print(\"Iterate tensor:\",k)\n","  print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JY9XNjcgoyvY","executionInfo":{"status":"ok","timestamp":1654596918196,"user_tz":240,"elapsed":327,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"63278e13-6391-42a3-d2c6-add4b03f1960"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iterate tensor: tf.Tensor(1, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(2, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(3, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(4, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(5, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(6, shape=(), dtype=int32)\n","\n","\n","Iterate tensor: tf.Tensor(7, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(8, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(9, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(10, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(11, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(12, shape=(), dtype=int32)\n","\n","\n","Iterate tensor: tf.Tensor(13, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(14, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(15, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(16, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(17, shape=(), dtype=int32)\n","Iterate tensor: tf.Tensor(18, shape=(), dtype=int32)\n","\n","\n"]}]},{"cell_type":"markdown","source":["Continuemos viendo ejemplos para tensores con valores iguales para todos los elementos.  "],"metadata":{"id":"80zBFj6toyXC"}},{"cell_type":"code","source":["# Crear un tensor con todos valores iguales a uno\n","ones_tensor = tf.ones(shape=(3, 2))\n","ones_tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBJYLZByRGFn","executionInfo":{"status":"ok","timestamp":1654588684363,"user_tz":240,"elapsed":12,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"c27e3566-bdc9-49b0-f7a9-bc4e6301df68"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n","array([[1., 1.],\n","       [1., 1.],\n","       [1., 1.]], dtype=float32)>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Crear un tensor con todos valores iguales a cero\n","zeros_tensor = tf.zeros(shape=(2, 3))\n","zeros_tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3RFa2dSTC6A","executionInfo":{"status":"ok","timestamp":1654588684363,"user_tz":240,"elapsed":11,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"d441b9c0-b5f2-454d-ea60-d71636351453"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n","array([[0., 0., 0.],\n","       [0., 0., 0.]], dtype=float32)>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Crear un tensor con todos valores iguales a un valor definido\n","value_tensor = tf.fill((3, 3), 7) # Este metodo requiere no definir nombres de parametro\n","value_tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpBxM4oGTc61","executionInfo":{"status":"ok","timestamp":1654588684364,"user_tz":240,"elapsed":11,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"ef91adbb-7dcc-48aa-806b-2f07902da549"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n","array([[7, 7, 7],\n","       [7, 7, 7],\n","       [7, 7, 7]], dtype=int32)>"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Finalmente, podemos crear tensores con valores aleatorios, lo cual es bastante util cuando queremos simular un subconjunto de datos. Por ejemplo, imaginemos que tenemos un conjunto de imágenes en un tensor con el shape (32, 224, 224, 3), donde:\n","\n","* 32 es el tamaño del batch (la cantidad de imágenes que ve una red neuronal en un momento dado).\n","* 224, 224 (las 2 primeras dimensiones) son el alto y el ancho de las imágenes en píxeles.\n","* 3 es el número de canales de color de la imagen (rojo, verde azul).\n","\n","Podemos simular este caso usando la clase `tf.random.Generator`."],"metadata":{"id":"TIrOu1tPDoex"}},{"cell_type":"code","source":["# Crear un tensor aleatorio con la distribución de probabilidades uniforme \n","# También podríamos usar random_batch.normal() en el caso de requerir una distribución normal\n","random_batch = tf.random.Generator.from_seed(42) # Establecer la semilla para la reproducibilidad\n","batch = random_batch.uniform(minval=0, maxval=255, shape=(32, 224, 224, 3),  dtype=tf.int32) # create tensor from a normal distribution \n","batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVz7hc0lX3ny","executionInfo":{"status":"ok","timestamp":1654588684707,"user_tz":240,"elapsed":6,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"1b0a62f6-3d11-423e-82ce-f08211af90e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(32, 224, 224, 3), dtype=int32, numpy=\n","array([[[[243,  42, 111],\n","         [ 48,  81,  66],\n","         [181, 207, 217],\n","         ...,\n","         [ 31, 141, 144],\n","         [ 80, 161,  96],\n","         [  4, 130, 124]],\n","\n","        [[ 61, 139,  86],\n","         [120,  30, 245],\n","         [107, 176, 156],\n","         ...,\n","         [ 20, 241,  13],\n","         [161, 215,  26],\n","         [147,  61,  88]],\n","\n","        [[ 28,  32,  98],\n","         [ 41, 207,  56],\n","         [203, 236, 168],\n","         ...,\n","         [ 83,  75, 112],\n","         [237, 251, 227],\n","         [224, 119, 108]],\n","\n","        ...,\n","\n","        [[215,  11, 182],\n","         [120,  91, 171],\n","         [  6,  93,  80],\n","         ...,\n","         [143,   7, 243],\n","         [ 28, 202, 193],\n","         [ 60, 214,  97]],\n","\n","        [[ 59, 211,  40],\n","         [254,  56,  57],\n","         [149, 214,  24],\n","         ...,\n","         [  2,  83, 169],\n","         [231,  80, 115],\n","         [ 86,  96, 194]],\n","\n","        [[151,   5, 208],\n","         [ 20, 156, 229],\n","         [137, 198, 115],\n","         ...,\n","         [146, 156, 234],\n","         [245, 191, 165],\n","         [ 38, 122, 157]]],\n","\n","\n","       [[[ 64, 166, 245],\n","         [133,  86, 231],\n","         [ 37, 104,  22],\n","         ...,\n","         [ 81, 102, 187],\n","         [239, 203,  98],\n","         [ 93,   6,  30]],\n","\n","        [[ 33,  99,   2],\n","         [189,  51,  37],\n","         [ 78,  71, 140],\n","         ...,\n","         [172,  62,  54],\n","         [203,  38,  71],\n","         [158, 231, 102]],\n","\n","        [[215,  38, 191],\n","         [189,   6,  77],\n","         [ 74, 149, 245],\n","         ...,\n","         [151, 185,  58],\n","         [126, 189,  57],\n","         [  6, 210, 199]],\n","\n","        ...,\n","\n","        [[  2, 248, 220],\n","         [180, 140, 222],\n","         [  3, 227, 205],\n","         ...,\n","         [119,  54, 253],\n","         [110,  21, 108],\n","         [186,  90,  99]],\n","\n","        [[166, 229,  61],\n","         [113,  50,  65],\n","         [ 73, 206, 192],\n","         ...,\n","         [ 50,  77, 179],\n","         [128,  75, 191],\n","         [ 74,  87, 203]],\n","\n","        [[248, 105,  18],\n","         [  6, 161, 151],\n","         [ 78,   7,  46],\n","         ...,\n","         [ 10,   9, 111],\n","         [174, 158, 198],\n","         [224,  73, 193]]],\n","\n","\n","       [[[ 18,  42,  66],\n","         [164, 146, 160],\n","         [ 80,  30, 139],\n","         ...,\n","         [ 11, 228,  43],\n","         [ 99, 158, 111],\n","         [ 56, 243, 219]],\n","\n","        [[216, 220, 148],\n","         [ 68,  60,   1],\n","         [ 60, 202, 137],\n","         ...,\n","         [254,  90, 227],\n","         [ 21, 128, 232],\n","         [231, 121, 125]],\n","\n","        [[ 17,  30, 229],\n","         [238, 136,  97],\n","         [191, 193, 172],\n","         ...,\n","         [164, 114, 151],\n","         [121, 162,  84],\n","         [ 49,  60, 252]],\n","\n","        ...,\n","\n","        [[ 84, 105, 240],\n","         [214, 123, 123],\n","         [ 76, 151, 164],\n","         ...,\n","         [235,  89,  17],\n","         [150,  82, 231],\n","         [123,  71,  60]],\n","\n","        [[115, 214, 209],\n","         [137,  21, 133],\n","         [  7,  86, 230],\n","         ...,\n","         [ 67,  47, 124],\n","         [140, 100, 175],\n","         [153, 177, 158]],\n","\n","        [[131, 238,  66],\n","         [239, 154, 190],\n","         [ 47, 116,  73],\n","         ...,\n","         [185, 174,  88],\n","         [137,  74, 154],\n","         [ 21,  75, 171]]],\n","\n","\n","       ...,\n","\n","\n","       [[[134, 161, 132],\n","         [234,  58, 175],\n","         [184, 188, 190],\n","         ...,\n","         [ 10, 165,  17],\n","         [243,  70,   4],\n","         [ 98, 103, 123]],\n","\n","        [[104, 110, 153],\n","         [199, 210,  38],\n","         [208, 197, 148],\n","         ...,\n","         [ 17, 204,  95],\n","         [125, 223, 201],\n","         [ 32, 254, 131]],\n","\n","        [[182, 202, 254],\n","         [109,  10, 138],\n","         [148, 153, 204],\n","         ...,\n","         [  5, 233, 165],\n","         [154, 135, 221],\n","         [ 34,  46,  20]],\n","\n","        ...,\n","\n","        [[ 96,   2, 162],\n","         [224,  38,  38],\n","         [160, 181, 239],\n","         ...,\n","         [104, 161, 148],\n","         [122, 223, 220],\n","         [ 33,  75,  42]],\n","\n","        [[ 87,  48, 211],\n","         [227,  42, 133],\n","         [250, 213, 105],\n","         ...,\n","         [254, 247,  24],\n","         [ 35, 184,  32],\n","         [ 32,  51,  30]],\n","\n","        [[  1,  56,  32],\n","         [211, 194, 201],\n","         [ 28,  62,  49],\n","         ...,\n","         [ 83, 132, 240],\n","         [101, 175,  35],\n","         [128, 236, 220]]],\n","\n","\n","       [[[ 17, 165,  53],\n","         [131, 100, 129],\n","         [117, 115, 119],\n","         ...,\n","         [  4, 192, 132],\n","         [120, 112,  88],\n","         [ 66,  49, 233]],\n","\n","        [[138, 224,  61],\n","         [115,  99,  83],\n","         [206,  74, 156],\n","         ...,\n","         [ 58, 136,  83],\n","         [229, 119, 187],\n","         [160,  63,  65]],\n","\n","        [[150, 103, 181],\n","         [144, 101, 128],\n","         [ 37, 156,  91],\n","         ...,\n","         [  4, 150, 173],\n","         [132,  88, 244],\n","         [ 30, 189, 108]],\n","\n","        ...,\n","\n","        [[244, 145, 112],\n","         [ 81,  98,  48],\n","         [ 16, 111,  60],\n","         ...,\n","         [136,  48, 120],\n","         [115,  29,  32],\n","         [ 74,  10,  71]],\n","\n","        [[175, 254,  76],\n","         [ 76, 242, 232],\n","         [195, 197, 233],\n","         ...,\n","         [163,  42, 155],\n","         [ 42, 208,  99],\n","         [ 72,  15,  81]],\n","\n","        [[  2, 193, 207],\n","         [  0,  62,  85],\n","         [198,  52,  25],\n","         ...,\n","         [215,  93,  70],\n","         [246, 126,  54],\n","         [226, 102,  37]]],\n","\n","\n","       [[[237, 240, 202],\n","         [147, 116,  77],\n","         [123, 119,  20],\n","         ...,\n","         [171, 175,  96],\n","         [ 54, 198,  19],\n","         [242,  72,  98]],\n","\n","        [[218,  13, 161],\n","         [142, 183,  46],\n","         [ 11,   8, 110],\n","         ...,\n","         [ 28, 250, 234],\n","         [163,  70, 103],\n","         [168, 250,  22]],\n","\n","        [[241, 114, 126],\n","         [135, 187,  50],\n","         [ 58, 194, 109],\n","         ...,\n","         [137, 131, 118],\n","         [ 27,  70,  83],\n","         [ 71, 113, 124]],\n","\n","        ...,\n","\n","        [[ 65,  50, 143],\n","         [ 79,  60,  82],\n","         [ 36, 176,  49],\n","         ...,\n","         [ 56,  87,  87],\n","         [ 99,  42, 183],\n","         [ 95, 252, 215]],\n","\n","        [[104, 134, 154],\n","         [  6,  92,  19],\n","         [ 92, 253,  32],\n","         ...,\n","         [151,   5,  19],\n","         [144,  87, 250],\n","         [221, 184,  92]],\n","\n","        [[245, 169, 239],\n","         [161, 208, 167],\n","         [ 24,   0, 236],\n","         ...,\n","         [ 17,  59, 210],\n","         [ 39, 178,  70],\n","         [140, 238, 250]]]], dtype=int32)>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["Para concluir esta sección, veremos que también podemos crear tensores desde arrays de Numpy. Dos principales maneras de hacer esto es pasando un array de Numpy a la función `tf.constant()` o `tf.Variable()`, y convirtiendo un array de Numpy con `tf.convert_to_tensor(array_numpy)`."],"metadata":{"id":"Jjo-R9MRioiC"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Crear una matriz NumPy entre 1 y 25\n","numpy_A = np.arange(1, 25, dtype=np.int32) \n","\n","A = tf.constant(numpy_A,  \n","                shape=[2, 4, 3]) # nota: el numero total de elementos de la forma (2*4*3) tiene que coincidir con el número de elementos en la matriz numpy_A\n","print(\"Array Numpy\")\n","print(numpy_A)\n","print(type(numpy_A)) \n","print(\"\\n\")\n","print(\"Tensor TF\")\n","print(A)\n","print(type(A))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwyYzQWJmSVw","executionInfo":{"status":"ok","timestamp":1654588685030,"user_tz":240,"elapsed":326,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"f9b01962-fd57-400f-9344-38f5aa143730"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Array Numpy\n","[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n","<class 'numpy.ndarray'>\n","\n","\n","Tensor TF\n","tf.Tensor(\n","[[[ 1  2  3]\n","  [ 4  5  6]\n","  [ 7  8  9]\n","  [10 11 12]]\n","\n"," [[13 14 15]\n","  [16 17 18]\n","  [19 20 21]\n","  [22 23 24]]], shape=(2, 4, 3), dtype=int32)\n","<class 'tensorflow.python.framework.ops.EagerTensor'>\n"]}]},{"cell_type":"code","source":["print(\"Retorno a Array Numpy\")\n","back_numpy = A.numpy()\n","\n","print(back_numpy)\n","print(type(back_numpy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29EUocQ2pmwM","executionInfo":{"status":"ok","timestamp":1654588685031,"user_tz":240,"elapsed":9,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"50b34b6b-7661-49ea-8d11-d2c5bf36e9bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Retorno a Array Numpy\n","[[[ 1  2  3]\n","  [ 4  5  6]\n","  [ 7  8  9]\n","  [10 11 12]]\n","\n"," [[13 14 15]\n","  [16 17 18]\n","  [19 20 21]\n","  [22 23 24]]]\n","<class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","source":["## 3. Operaciones con Tensores"],"metadata":{"id":"3wETwEiyL-Xs"}},{"cell_type":"markdown","source":["### 3.1 Operaciones básicas\n","Las operaciones básicas como la suma, resta, multiplicación y división entre un tensor y un valor escalar son aplicables directamente con signos respectivos. Por ejemplo:  "],"metadata":{"id":"LAv0LFS-rDfH"}},{"cell_type":"code","source":["# Agregar valores a un tensor usando el operador de suma\n","tensor = tf.Variable([[10, 7], [3, 4]])\n","tensor = tensor + 10\n","tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okrEZX9Lswxq","executionInfo":{"status":"ok","timestamp":1654588685031,"user_tz":240,"elapsed":6,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"f2ebb714-1a1f-45e8-dcb3-c53c6fd578a4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n","array([[20, 17],\n","       [13, 14]], dtype=int32)>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["TF también tiene funciones para realizar las mismas operaciones de manera optimizada: `tf.multiply()`, `tf.add()`, `tf.substract()`, `tf.divide()`. Se sugiere priorizar el uso de estas funciones cada vez que se pueda. Estas funciones tienen la ventaja de ejecutarse de manera optimizada cuando se esta trabajando con modelos y estos forman parte de un [gráfico de TensorFlow](https://www.tensorflow.org/tensorboard/graphs)."],"metadata":{"id":"HAmpJu8BtGWT"}},{"cell_type":"code","source":["# Usar la función equivalente al operador '*' (multiplicar)\n","tensor = tf.multiply(tensor, 20)\n","tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hyF3S_h3r4LP","executionInfo":{"status":"ok","timestamp":1654588685031,"user_tz":240,"elapsed":4,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"1f2696a3-0ae3-41ea-f4f3-714386845f45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n","array([[400, 340],\n","       [260, 280]], dtype=int32)>"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["### 3.2 Multiplicación de Matrices\n","\n","Una de las operaciones más comunes en los algoritmos de aprendizaje automático es la multiplicación de matrices. TensorFlow implementa esta operación con el método `tf.matmul()`. Las dos reglas principales para realizar la multiplicación de matrices (simbolizada por @) son: \n","\n","1. Las dimensiones internas de las matrices deben coincidir.\n","  - (3, 5) @ (3, 5) no funcionara \n","  - (5, 3) @ (3, 5) funciona\n","  - (3, 5) @ (5, 3) funciona\n","2. La matriz resultante tiene la forma de las dimensiones exteriores.\n","  - (5, 3) @ (3, 5) -> (5, 5)\n","  - (3, 5) @ (5, 3) -> (3, 3)"],"metadata":{"id":"WgmuKwHkwjWd"}},{"cell_type":"code","source":["print(tensor)\n","tensor = tf.matmul(tensor, tensor)\n","print(tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y49CkXAiwjBX","executionInfo":{"status":"ok","timestamp":1654588685031,"user_tz":240,"elapsed":3,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"cc3e624f-3d5a-42d2-e4ce-8396e33187ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[400 340]\n"," [260 280]], shape=(2, 2), dtype=int32)\n","tf.Tensor(\n","[[248400 231200]\n"," [176800 166800]], shape=(2, 2), dtype=int32)\n"]}]},{"cell_type":"code","source":["# Python también tiene un operador directo para realizar multiplicación de matrices: @ \n","c = tf.Variable([[1.0, 2.0], [3.0, 4.0]])\n","d = tf.Variable([[1.0, 1.0], [0.0, 1.0]])\n","e = c @ d\n","print(e)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMq65KmYyJ77","executionInfo":{"status":"ok","timestamp":1654588687236,"user_tz":240,"elapsed":2207,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"6d7e2dd8-e8e3-4f11-ae97-02c6d7026a38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[1. 3.]\n"," [3. 7.]], shape=(2, 2), dtype=float32)\n"]}]},{"cell_type":"markdown","metadata":{"id":"KZ9BxPTv6HLz"},"source":["### 3.3 Cambiar la forma de tensores\n","\n","Remodelar o *Reshape* tensores es una operación muy común. Primero verificamos el tamaño actual de un tensor con `tf.size()`. Luego, para remodelar un tensor, usamos una de las siguientes funciones: \n","- `tf.reshape()` para remodelar un tensor a una forma objetivo.\n","- `tf.transpose()` para intercambiar las dimensiones de un tensor dado, también conocido como transponer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GHOtN0n6HL0","outputId":"7f5db433-59be-4606-d7f0-86378ba3164e","executionInfo":{"status":"ok","timestamp":1654733092857,"user_tz":240,"elapsed":302,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.Variable 'Variable:0' shape=(3, 2) dtype=int32, numpy=\n","array([[ 7,  8],\n","       [ 9, 10],\n","       [11, 12]], dtype=int32)>\n","tf.Tensor(\n","[[ 27  30  33]\n"," [ 61  68  75]\n"," [ 95 106 117]], shape=(3, 3), dtype=int32)\n","tf.Tensor(\n","[[ 27  30  33]\n"," [ 61  68  75]\n"," [ 95 106 117]], shape=(3, 3), dtype=int32)\n"]}],"source":["# Crea un tensor (3,2)\n","# Nota: Una practica usual es denominar a las matrices con una mayuscula y a los vectores con letras minusculas\n","X = tf.Variable([[1, 2],\n","                 [3, 4],\n","                 [5, 6]])\n","\n","# Crear otro tensor (3, 2) \n","Y = tf.Variable([[7, 8],\n","                 [9, 10],\n","                 [11, 12]])\n","\n","# Multiplicar X y Y causara errores debido a que sus dimensiones internas no tienen el mismo valor\n","# Por esa razón, es necesario aplicar reshape\n","Z = tf.reshape(Y, shape=(2, 3))\n","result = tf.matmul(X, Z)\n","print(result)\n","\n","# Intentemos con la función tf.transpose()\n","T = tf.transpose(Y)\n","result_transpose = tf.matmul(X, Z)\n","print(result_transpose)"]},{"cell_type":"markdown","source":["### 3.4 El producto punto\n","\n","Otra operación muy utilizada en redes neuronales es el [producto punto](https://es.wikipedia.org/wiki/Producto_escalar#:~:text=Algebraicamente%2C%20el%20producto%20punto%20es,coseno%20del%20%C3%A1ngulo%20entre%20ellos.), el cual es la suma de los productos de las correspondientes entradas en dos secuencias de números. Geométricamente, es el producto de dos magnitudes euclidianas de los dos vectores y el coseno del ángulo entre ellos. Esta operación se puede implementar con la funcion `tf.tensordot()` especificando el eje=2. "],"metadata":{"id":"uOy8N4m2EvZn"}},{"cell_type":"code","source":["dotproduct = tf.tensordot(X, Z, axes = 2)\n","dotproduct"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R96v0U4mE0wp","executionInfo":{"status":"ok","timestamp":1654588687237,"user_tz":240,"elapsed":20,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"2152c9ae-461d-42a3-acfd-2dfd07c51f3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=int32, numpy=217>"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### 3.5 Exprimir un tensor (eliminar todas las dimensiones iguales a la unidad)\n","\n","Un problema común al procesar conjuntos de datos es la carga de datasets y que la estructura de datos en la que se carguen tenga dimensiones con valor 1. Si necesita eliminar estas dimensiones  de un tensor, puede usar `tf.squeeze()`."],"metadata":{"id":"GZc1JazDG554"}},{"cell_type":"code","source":["# Crear un tensor en la quinta dimensión con 20 elementos entre el 0 y el 20\n","tensor = tf.Variable(initial_value=[[[[np.random.randint(0, 20, 20)]]]], shape=(1, 1, 1, 1, 20))\n","print(tensor.shape)\n","print(len(tensor.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bG2Pk5dLG5gr","executionInfo":{"status":"ok","timestamp":1654588687237,"user_tz":240,"elapsed":18,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"d751bea6-c506-46a9-8b06-5af5f8f40347"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 1, 1, 1, 20)\n","5\n"]}]},{"cell_type":"code","source":["# Exprimir el tensor (eliminar todas las dimensiones con valor 1)\n","tensor_squeezed = tf.squeeze(tensor)\n","print(tensor_squeezed.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YIxFYVhHHntY","executionInfo":{"status":"ok","timestamp":1654588750601,"user_tz":240,"elapsed":3,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"66c2e5f7-a3bc-4d28-9d4f-3ef4caa1417c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(20,)\n"]}]},{"cell_type":"markdown","source":["## 4. Acceder a la GPU\n","\n","Como se menciono anteriormente, acceder a la GPU trae muchos beneficios para el entrenamiento e inferencia de modelos. Puedes comprobar si tienes acceso a la GPU en tu equipo local o en la nube usando la siguiente instrucción: \n"],"metadata":{"id":"9kD3qelTLhN9"}},{"cell_type":"code","source":["print(tf.config.list_physical_devices('GPU'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bia_CQTHMMsn","executionInfo":{"status":"ok","timestamp":1654589019295,"user_tz":240,"elapsed":380,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"b9f8a28e-bc53-4258-b3e4-e2757d8eaec9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}]},{"cell_type":"markdown","source":["Si la anterior celda genera una matriz vacía (o nada), significa que no tienes acceso a la GPU (o al menos que TensorFlow no se puede conectar). Colab dispone de GPU para todos los usuarios, por lo que deberías asegurarte que la GPU esta activada de esta manera:\n","\n","> Seleccionando en la barra superior de funciones *Runtime -> Change Runtime Type -> Select GPU* \n","\n","Una vez que haya cambiado el Runtime Type, ejecute la anterior celda para verificar que el la GPU ya esta accesible. Un vez ya este activa, TF se asegurara de usarla siempre que se pueda. "],"metadata":{"id":"VUwiC00YMRNZ"}},{"cell_type":"code","source":["import tensorflow as tf\n","print(tf.config.list_physical_devices('GPU'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4i57Gk7MQUH","executionInfo":{"status":"ok","timestamp":1654589236524,"user_tz":240,"elapsed":7,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"b4f47f0e-0077-4ec2-cc46-ae505c880569"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}]},{"cell_type":"markdown","source":["Finalmente, puedes acceder a mas información sobre la GPU usando la siguiente instrucción de consola."],"metadata":{"id":"i809NdvyNbzr"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFAWrF1YNi1q","executionInfo":{"status":"ok","timestamp":1654733547216,"user_tz":240,"elapsed":322,"user":{"displayName":"Edwin Salcedo","userId":"01626262771981202003"}},"outputId":"8b097910-7618-47b6-9498-9ab5979651cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jun  9 00:12:26 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    33W / 250W |    377MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["##**Ejercicio 1:** Trabajando con Tensores\n","\n","1. Crear tres tensores que contengan valores aleatorios entre 0 y 20 con forma [5, 300, 3], mostrar estos tensores y mostrar los elementos que se repiten en los tres.\n","2. Crear dos tensores con valores aleatorios entre 0 y 1 con forma [224, 224, 3], y encontrar su producto punto. \n","3. Encuentrar los valores mínimo y máximo de los tensores que creó en el anterior punto.\n","4. Crear un tensor con valores aleatorios entre el 0 y 1 usando una la distribución uniforme y con la forma [1, 224, 224, 3]. Luego exprimirlo para cambiar a la forma a [224, 224, 3].\n","5. Crear 5 imágenes de 5x5 con Numpy y transformarlas en un tensor con las dimensiones [5,1,1,5,5]. "],"metadata":{"id":"5dZuSOxjPmyc"}},{"cell_type":"code","source":[""],"metadata":{"id":"8PUFbO9uQesU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Ejercicio 2:** Registro de asistencia\n","Este ejercicio requiere modelar el registro y contabilización de asistencias en una escuela usando tensores de TensorFlow. Puede inventar una lista de 15 estudiantes que participaron 60 días de clase durante un trimestre. Aunque no todos los estudiantes asistieron los 60 días, usted tiene el registro de cuando asistieron y cuando no. Su tarea sera contabilizar los siguientes puntos: \n","- ¿Cual fue el estudiante que tuvo mas faltas?\n","- ¿Cuantas asistencias tiene cada estudiante?\n","- ¿Cual es el promedio de asistencia en el curso?\n","- ¿Que estudiante tuvo menos faltas? "],"metadata":{"id":"Kd10ynWrQfBZ"}},{"cell_type":"code","source":[""],"metadata":{"id":"JS6c_KLOSb79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Referencias\n","- [Introducción a los tensores ](https://www.tensorflow.org/guide/tensor)\n","- [Variables en TensorFlow](https://www.tensorflow.org/guide/variable)\n","- [How to Create Tensors with Known Values?](https://www.dummies.com/article/technology/information-technology/ai/machine-learning/create-tensors-known-values-253479/) \n","- [TensorFlow Basics](https://www.tensorflow.org/guide/basics)\n","- [Examinando el gráfico de TensorFlow](https://www.tensorflow.org/tensorboard/graphs) "],"metadata":{"id":"spJNioJTZELC"}}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"Laboratorio 9: Introducción a TensorFlow.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":0}